{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdyXUVFi7VXk"
   },
   "source": [
    "##SENTIMEN ANALISIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9BNjLXnhSrP"
   },
   "source": [
    "#Modul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2PnYes68ca1y",
    "outputId": "db242ee7-227a-4265-8911-afbdf37f06ce"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate wordcloud scikit-learn nltk\n",
    "!pip install Sastrawi\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kydvx31shVXH"
   },
   "source": [
    "#Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOk2GgLXchMH",
    "outputId": "05bac40e-a9ed-4968-c50c-38ae55dfeec3"
   },
   "outputs": [],
   "source": [
    "# Load Data Train (Udah Bersih, Udah Ada Label)\n",
    "Train = \"https://raw.githubusercontent.com/RakhaFS/TA-AMS/refs/heads/main/Train%20Shell.csv\"\n",
    "Test = \"https://raw.githubusercontent.com/RakhaFS/TA-AMS/refs/heads/main/Shell%20Indonesia.csv\"\n",
    "\n",
    "Shell_tr = pd.read_csv(Train)  # Ganti nama sesuai file lo\n",
    "train = Shell_tr[['favorite_count', 'full_text', 'clean_text', 'sentiment']]  # Pastikan kolom sesuai\n",
    "\n",
    "# Load Data Test (Data Mentah)\n",
    "df = pd.read_csv(Test)\n",
    "df = df[['favorite_count', 'full_text']].dropna().reset_index(drop=True)\n",
    "\n",
    "print('Data Train sebanyak ', len(train))\n",
    "print('Data Test sebanyak ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gz66R_Z_aDrW",
    "outputId": "423b9da1-9dc0-4945-fe60-958a94767a07"
   },
   "outputs": [],
   "source": [
    "po = train['sentiment'].str.contains('positive')\n",
    "nt = train['sentiment'].str.contains('neutral')\n",
    "ng = train['sentiment'].str.contains('negative')\n",
    "# Tampilkan hasilnya\n",
    "\n",
    "jpo = po.sum()\n",
    "jnt = nt.sum()\n",
    "jng = ng.sum()\n",
    "print(f\"Jumlah positive: {jpo}\")\n",
    "print(f\"Jumlah neutral: {jnt}\")\n",
    "print(f\"Jumlah negative: {jng}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5_ZIv2-huoV"
   },
   "source": [
    "#Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "VuDEfKp9hNsC",
    "outputId": "6c2a6aac-6513-427e-9ad3-1810060d0924"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "custom_stopwords = {'yg', 'aja', 'dong', 'nih', 'gk', 'kok', 'sih', 'loh', 'lah', 'nya'}\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "df['clean_text'] = df['full_text'].apply(preprocess).apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "pI5ZHlecC29v",
    "outputId": "953eea31-e9c9-41f8-f7e8-cb260fd470c9"
   },
   "outputs": [],
   "source": [
    "# Pastikan kolom 'clean_text' dan 'sentiment' sudah ada\n",
    "assert 'clean_text' in train.columns\n",
    "assert 'sentiment' in train.columns\n",
    "\n",
    "# Encode label jadi angka (positive = 2, neutral = 1, negative = 0 misalnya)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['sentiment'])\n",
    "\n",
    "# Load test data juga, dari shell.csv yang tadi udah di-clean dan belum dilabeli\n",
    "df_test = df[~df.index.isin(train.index)].reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp3K2FD8hXNL"
   },
   "source": [
    "#Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357,
     "referenced_widgets": [
      "a65e8207dacf45bb88e577f95795cb16",
      "9e5526efb60447f58fca5ed7eda8d0e7",
      "04ef4dabbad34c3da99810248d12ed93",
      "12923d64748f40789582a9218eab083d",
      "f801bb3546394a7e8cabc10f83f6d085",
      "6c0511936c5845fe999fa25053376a47",
      "6e2eff9437f344c18d3ab428bebce1a6",
      "f3cad74029ba494090385c5f296ebf6e",
      "4b0431316b3c42dcaf8c3218e0491d09",
      "1709fe435ee94f9096edf15987a9f415",
      "38ed35baee764e7dad6f0365f4d862ab",
      "dcbc0575736346a2b56c6bcf6892795b",
      "f6bf303ddbdf4c76a36757a62985ffaf",
      "d1754192ab904f14adf1011423b01c7d",
      "f075273cbf2642cb99898384b1411966",
      "73a73fbe5c4f4812917bf7d058e8c3f9",
      "35dde109573a48669d872ac93f50b62e",
      "725bab287a994e1192b272ff297c2318",
      "643e4d98ba49443ab332a46e717a27e8",
      "a229bf13053045aa95056009fda286d8",
      "186aa3d1065a41d99539453a6598a414",
      "965d258fccb74631a0874423dbe5ee87",
      "dea8b6d288854038adf66673d2431dc2",
      "1b002d57af574292aedefb4c2e35d9cd",
      "f0d079ce8e31404b9fcacf2b3ee75811",
      "c4dff4ab0b284ff1a691675f991c0f51",
      "eb3d1141e74247a29481ab2ee03a7def",
      "07c51cb124ea4efe997ea9a5072b8b07",
      "7f7cd88ed7254a0d90de5be338ae1c23",
      "e2a89da6c5354438a716f91a485485c9",
      "3fb9ae06ff82486dac56714856f2ad72",
      "fa074f17123f4d0b904dd92f2e321147",
      "3d1e2df1a75a4de093a9fb56cdbde312",
      "14531521c72d470fa1182fa0cbc240b1",
      "1e0b6a3a10594fb199e3427101e77ce1",
      "2e60a323b857460c92b164661deac477",
      "ac4a3a0d0c374cdb8cd4fc11493d7da4",
      "8e84d49291ea462c8a16ecff4bae076c",
      "a73cdd3119c8461dbf86ed6fe8390267",
      "eacd8d068cb2467fb1be1d2a73533f62",
      "845629e743a245a28c78b2d581ace0e9",
      "1ed0aad414a94bf5aa7bba63de709881",
      "b85d36192e3341d5bdf39e9c00c5ae89",
      "ea1ce273a2d147a3bf51e373a67e9ab2",
      "6e979dde233449b8be07b8c49504d421",
      "b5fdf3c847f34f789b345b2edca33233",
      "5dff8dae098a4223b6ee1a5e44f706cc",
      "bbe05e3fd60d44ac98b8663c59a504b6",
      "9c8e0f488a7943abb1c2b814f1b4388b",
      "ce94d917aa5b469f854000f7d539c33d",
      "b0d6863401ed450eb28a63df0d573dd7",
      "47ffa655db6d40f5b98b10c4b3dd2905",
      "09bf9239e6fd43228515fc9884ec4104",
      "1424995bb0e94a9da6a2ced9e52ba2e0",
      "d62797966c5440e8bbf3c14e5b699645",
      "5ce72a0f4d8e46e591c6488450d553c2",
      "8591b326dd904aeca67baa62b98e8862",
      "063778e6dc1a4a1cb54101a6c6067da1",
      "15410d3087714a78a4bd1cd1990f6e00",
      "be540d0529da4d32ad33305780be0a4a",
      "88cbfbe0374c4834bf6df61e4efbb12a",
      "d3997092cc3a4c6db6afce01102dbb7c",
      "49071faf51c743e4a2571299b8fd9bac",
      "2569aa57640b490f98e6594ff48412e1",
      "4f606487665e4a3ea37fb2ad94c33863",
      "db85c23037d2411493c6b90ab61a48f4"
     ]
    },
    "id": "n8MvhO1ldAQe",
    "outputId": "c23d2e1d-a3d3-42ce-8903-22fa69846949"
   },
   "outputs": [],
   "source": [
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['clean_text'], truncation=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train[['clean_text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(df[['clean_text']])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "id": "uCaTvJpMdF2j",
    "outputId": "8e5783a5-be89-4578-fd61-6309e1082363"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkTNB_ZLh6G2"
   },
   "source": [
    "#Sentiment Analysis dengan BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sj1703wEdKTY",
    "outputId": "006c8870-59c4-4cda-d2b3-bc70d0a24854"
   },
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "df['sentiment'] = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "# Export CSV hasil\n",
    "df[['favorite_count', 'full_text', 'clean_text', 'sentiment']].to_csv(\"hasil_finetune_Shell.csv\", index=False)\n",
    "print(\"✅ Hasil disimpan ke 'hasil_finetune_Shell.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "C2fKqNCMjm9x",
    "outputId": "21ced547-673a-4025-cdd5-03ee47093380"
   },
   "outputs": [],
   "source": [
    "look = df[['full_text','clean_text','sentiment']]\n",
    "look.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
